{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck3rO-WkRhiF"
      },
      "source": [
        "# Training Abstract2Title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:13:20.031848Z",
          "iopub.status.busy": "2021-11-19T13:13:20.031498Z",
          "iopub.status.idle": "2021-11-19T13:13:23.357558Z",
          "shell.execute_reply": "2021-11-19T13:13:23.356948Z",
          "shell.execute_reply.started": "2021-11-19T13:13:20.031781Z"
        },
        "id": "fOHBMr2IRhiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430160bf-967a-41c0-bae4-d333c579ce84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.7.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_from_disk, load_metric\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, \\\n",
        "                         DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ],
      "metadata": {
        "id": "6JqDAWLEUNdw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/arxiv_AI_dataset.zip"
      ],
      "metadata": {
        "id": "j9V3HkVtSkfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p3EV1bMRhiK",
        "outputId": "9bc92f0d-191c-4b77-e09c-6e7a7debfef7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-_KWB26RhiL"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:13:31.687885Z",
          "iopub.status.busy": "2021-11-19T13:13:31.687541Z",
          "iopub.status.idle": "2021-11-19T13:13:31.997391Z",
          "shell.execute_reply": "2021-11-19T13:13:31.996752Z",
          "shell.execute_reply.started": "2021-11-19T13:13:31.687860Z"
        },
        "scrolled": true,
        "tags": [],
        "id": "2HwLB65ORhiL"
      },
      "outputs": [],
      "source": [
        "# Initialize T5-base tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('t5-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:13:33.471738Z",
          "iopub.status.busy": "2021-11-19T13:13:33.471420Z",
          "iopub.status.idle": "2021-11-19T13:13:33.482045Z",
          "shell.execute_reply": "2021-11-19T13:13:33.481524Z",
          "shell.execute_reply.started": "2021-11-19T13:13:33.471714Z"
        },
        "id": "vMd1lypyRhiL"
      },
      "outputs": [],
      "source": [
        "# Load the processed data\n",
        "dataset = load_from_disk('arxiv_AI_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:13:34.314993Z",
          "iopub.status.busy": "2021-11-19T13:13:34.314678Z",
          "iopub.status.idle": "2021-11-19T13:13:34.317945Z",
          "shell.execute_reply": "2021-11-19T13:13:34.317360Z",
          "shell.execute_reply.started": "2021-11-19T13:13:34.314969Z"
        },
        "id": "fgT0r27aRhiM"
      },
      "outputs": [],
      "source": [
        "MAX_SOURCE_LEN = 512\n",
        "MAX_TARGET_LEN = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:13:34.969829Z",
          "iopub.status.busy": "2021-11-19T13:13:34.969512Z",
          "iopub.status.idle": "2021-11-19T13:13:34.974526Z",
          "shell.execute_reply": "2021-11-19T13:13:34.973868Z",
          "shell.execute_reply.started": "2021-11-19T13:13:34.969805Z"
        },
        "id": "XmRXjb3URhiM"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(example):\n",
        "    \n",
        "    model_inputs = tokenizer(example['abstract'], max_length=MAX_SOURCE_LEN, padding=True, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(example['title'], max_length=MAX_TARGET_LEN, padding=True, truncation=True)\n",
        "\n",
        "    # Replace all pad token ids in the labels by -100 to ignore padding in the loss\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    model_inputs['labels'] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:13:36.373210Z",
          "iopub.status.busy": "2021-11-19T13:13:36.372893Z",
          "iopub.status.idle": "2021-11-19T13:13:37.342402Z",
          "shell.execute_reply": "2021-11-19T13:13:37.341857Z",
          "shell.execute_reply.started": "2021-11-19T13:13:36.373185Z"
        },
        "id": "KmsQ0JoKRhiN",
        "outputId": "5cba448a-088e-4663-d55d-390c9528a2e8",
        "colab": {
          "referenced_widgets": [
            "bbf771efd9544a3dbb27dc46ed6e90ef"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at arxiv_AI_dataset/train/cache-f2dcdb2e692d508c.arrow\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbf771efd9544a3dbb27dc46ed6e90ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Running tokenizer on dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at arxiv_AI_dataset/val/cache-e1ffc110fb199715.arrow\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['attention_mask', 'input_ids', 'labels'],\n",
              "        num_rows: 36074\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['attention_mask', 'input_ids', 'labels'],\n",
              "        num_rows: 2005\n",
              "    })\n",
              "    val: Dataset({\n",
              "        features: ['attention_mask', 'input_ids', 'labels'],\n",
              "        num_rows: 2004\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply preprocess_data() to the whole dataset\n",
        "processed_dataset = dataset.map(\n",
        "    preprocess_data,\n",
        "    batched=True,\n",
        "    remove_columns=['abstract', 'title'],\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "processed_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV67BaVLRhiN"
      },
      "source": [
        "## Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:13:53.967268Z",
          "iopub.status.busy": "2021-11-19T13:13:53.966933Z",
          "iopub.status.idle": "2021-11-19T13:13:53.970713Z",
          "shell.execute_reply": "2021-11-19T13:13:53.970086Z",
          "shell.execute_reply.started": "2021-11-19T13:13:53.967244Z"
        },
        "id": "KCob4XfoRhiN"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "num_epochs = 5\n",
        "learning_rate = 5.6e-5\n",
        "weight_decay = 0.01\n",
        "log_every = 50\n",
        "eval_every = 1000\n",
        "lr_scheduler_type = \"linear\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:14:00.354699Z",
          "iopub.status.busy": "2021-11-19T13:14:00.354389Z",
          "iopub.status.idle": "2021-11-19T13:14:00.451642Z",
          "shell.execute_reply": "2021-11-19T13:14:00.451017Z",
          "shell.execute_reply.started": "2021-11-19T13:14:00.354676Z"
        },
        "id": "09NrCW90RhiO"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"model-t5-base\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=eval_every,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=weight_decay,\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=num_epochs,\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=log_every,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    resume_from_checkpoint=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIZhwtQRhiO"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:14:04.988628Z",
          "iopub.status.busy": "2021-11-19T13:14:04.988287Z",
          "iopub.status.idle": "2021-11-19T13:14:13.093681Z",
          "shell.execute_reply": "2021-11-19T13:14:13.093052Z",
          "shell.execute_reply.started": "2021-11-19T13:14:04.988588Z"
        },
        "id": "aDq6G6hnRhiO"
      },
      "outputs": [],
      "source": [
        "# Initialize T5-base model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained('t5-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:14:13.095116Z",
          "iopub.status.busy": "2021-11-19T13:14:13.094858Z",
          "iopub.status.idle": "2021-11-19T13:14:13.252834Z",
          "shell.execute_reply": "2021-11-19T13:14:13.252320Z",
          "shell.execute_reply.started": "2021-11-19T13:14:13.095096Z"
        },
        "id": "DQZ_9vCeRhiP"
      },
      "outputs": [],
      "source": [
        "# Define ROGUE metrics on evaluation data\n",
        "metric = load_metric(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    \n",
        "    # Replace -100 in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # ROUGE expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    # Compute ROUGE scores and get the median scores\n",
        "    result = metric.compute(\n",
        "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        "    )\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:14:13.253756Z",
          "iopub.status.busy": "2021-11-19T13:14:13.253563Z",
          "iopub.status.idle": "2021-11-19T13:14:13.256642Z",
          "shell.execute_reply": "2021-11-19T13:14:13.256048Z",
          "shell.execute_reply.started": "2021-11-19T13:14:13.253738Z"
        },
        "id": "Blnb8YTLRhiP"
      },
      "outputs": [],
      "source": [
        "# Dynamic padding in batch using a data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:26:57.569948Z",
          "iopub.status.busy": "2021-11-19T13:26:57.569634Z",
          "iopub.status.idle": "2021-11-19T13:26:57.580471Z",
          "shell.execute_reply": "2021-11-19T13:26:57.579864Z",
          "shell.execute_reply.started": "2021-11-19T13:26:57.569925Z"
        },
        "tags": [],
        "id": "cSB3RowwRhiP"
      },
      "outputs": [],
      "source": [
        "# Define the trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=processed_dataset[\"train\"],\n",
        "    eval_dataset=processed_dataset[\"val\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "u8gskWCTRhiQ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Xcv-rJRhiQ"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:27:03.756757Z",
          "iopub.status.busy": "2021-11-19T13:27:03.756436Z",
          "iopub.status.idle": "2021-11-19T13:30:44.492437Z",
          "shell.execute_reply": "2021-11-19T13:30:44.491895Z",
          "shell.execute_reply.started": "2021-11-19T13:27:03.756734Z"
        },
        "scrolled": true,
        "tags": [],
        "id": "OoQhTtAJRhiQ",
        "outputId": "b4c30d2e-cabe-4630-b146-e6f096beec66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 2005\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='251' max='251' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [251/251 03:37]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 40s, sys: 194 ms, total: 3min 40s\n",
            "Wall time: 3min 40s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.6707532405853271,\n",
              " 'eval_rouge1': 47.1746,\n",
              " 'eval_rouge2': 26.8231,\n",
              " 'eval_rougeL': 41.7727,\n",
              " 'eval_rougeLsum': 41.8263,\n",
              " 'eval_runtime': 220.717,\n",
              " 'eval_samples_per_second': 9.084,\n",
              " 'eval_steps_per_second': 1.137}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "trainer.evaluate(eval_dataset=processed_dataset['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfl5K13DRhiR"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:40:10.661400Z",
          "iopub.status.busy": "2021-11-19T13:40:10.661056Z",
          "iopub.status.idle": "2021-11-19T13:40:10.665897Z",
          "shell.execute_reply": "2021-11-19T13:40:10.665328Z",
          "shell.execute_reply.started": "2021-11-19T13:40:10.661377Z"
        },
        "tags": [],
        "id": "Yen1VAI9RhiR"
      },
      "outputs": [],
      "source": [
        "temperature = 0.9\n",
        "num_beams = 4\n",
        "max_gen_length = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-11-19T13:40:33.699612Z",
          "iopub.status.busy": "2021-11-19T13:40:33.699293Z",
          "iopub.status.idle": "2021-11-19T13:40:33.956567Z",
          "shell.execute_reply": "2021-11-19T13:40:33.955828Z",
          "shell.execute_reply.started": "2021-11-19T13:40:33.699588Z"
        },
        "tags": [],
        "id": "LxnZIN-2RhiR",
        "outputId": "6ebe0644-1864-4bd2-f324-42a49f3d8954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Self-Supervised Learning of Vision Transformers\n"
          ]
        }
      ],
      "source": [
        "abstract = \"\"\"In this paper, we question if self-supervised learning provides\n",
        "new properties to Vision Transformer (ViT) [19] that\n",
        "stand out compared to convolutional networks (convnets).\n",
        "Beyond the fact that adapting self-supervised methods to this\n",
        "architecture works particularly well, we make the following\n",
        "observations: first, self-supervised ViT features contain\n",
        "explicit information about the semantic segmentation of an\n",
        "image, which does not emerge as clearly with supervised\n",
        "ViTs, nor with convnets. Second, these features are also excellent\n",
        "k-NN classifiers, reaching 78.3% top-1 on ImageNet\n",
        "with a small ViT. Our study also underlines the importance of\n",
        "momentum encoder [33], multi-crop training [10], and the\n",
        "use of small patches with ViTs. We implement our findings\n",
        "into a simple self-supervised method, called DINO, which\n",
        "we interpret as a form of self-distillation with no labels.\n",
        "We show the synergy between DINO and ViTs by achieving\n",
        "80.1% top-1 on ImageNet in linear evaluation with ViT-Base\"\"\"\n",
        "# abstract = dataset['test'][0]['abstract']\n",
        "inputs = tokenizer([abstract], max_length=512, return_tensors='pt')\n",
        "\n",
        "title_ids = model.generate(\n",
        "    inputs['input_ids'].to('cuda'), \n",
        "    num_beams=num_beams, \n",
        "    temperature=temperature, \n",
        "    max_length=max_gen_length, \n",
        "    early_stopping=True\n",
        ")\n",
        "title = tokenizer.decode(title_ids[0].tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "print(title)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "60fbf1aecf0122793952a73a80d27bc8732eff9e143c13520ca117508929b1c7"
    },
    "kernelspec": {
      "display_name": "PyTorch 1.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}